{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2e239a3",
   "metadata": {},
   "source": [
    "# Amazon Bestsellers WebScrapping project\n",
    "### The aim of this project is to scrape the urls of the different categories of books available in Amazon.in/books page and to get top 50 books under each category\n",
    "\n",
    "#### Steps to Achieve this\n",
    "- Scrape the Category name and category urls from the book page\n",
    "- Save the result in the form of csv file\n",
    "- Write a function to scrape all the relevant book info from each url\n",
    "- Write a function to save info from each category as a separate csv file and save everything in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b67482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will first import all the relevant libraries.\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271bdc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The URL of the main page from which we will start working on the project\n",
    "bestseller_url = 'https://www.amazon.in/gp/bestsellers/books/ref=zg_bs_unv_books_1_1318104031_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edb1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now define a function to scape all the URLs and Title of categories present on the Page\n",
    "\n",
    "def scrape_category_info(URL):\n",
    "    response = requests.get(URL)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to Scrape {}'.format(URL))\n",
    "    #We use Beautifulsoup to read the html\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "    category_tag = soup.find_all('div' , {'class': '_p13n-zg-nav-tree-all_style_zg-browse-item__1rdKf _p13n-zg-nav-tree-all_style_zg-browse-height-large__1z5B8'})\n",
    "    category_dict =[{ \n",
    "    'name' : cat.find('a').text,\n",
    "    'url' : 'https://amazon.in' + cat.find('a')['href']} for cat in category_tag[1:]]\n",
    "    return pd.DataFrame(category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2879f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function to do this at once and store it in a pd dataframe and save it as csv as well\n",
    "def book_info(category_url):\n",
    "    page = requests.get(category_url)\n",
    "\n",
    "\n",
    "    if page.status_code != 200:\n",
    "        raise Exception(\"Failed to download{}. Skipping...\".format(category_url))\n",
    "    topic_doc = bs(page.text, 'html.parser')\n",
    "    #Searching for tags which contain the information we need\n",
    "    book_name_tags = topic_doc.find_all('div', {'class': '_cDEzb_p13n-sc-css-line-clamp-1_1Fn1y'})\n",
    "    book_rating_tag = topic_doc.find_all('div', {'class': 'a-icon-row'})\n",
    "    book_type_tag = topic_doc.find_all('div', {'class': 'a-row a-size-small'})\n",
    "    book_price_tag = topic_doc.find_all('div', {'class': 'a-row'})\n",
    "\n",
    "    #Creating empty lists\n",
    "    book_name = []\n",
    "    book_author = []\n",
    "    book_ratings = []\n",
    "    book_star = []\n",
    "    book_type = []\n",
    "    book_price = []\n",
    "    #loops to save information\n",
    "    for i in range(len(book_name_tags)):\n",
    "        if i % 2 == 0:\n",
    "            book_name.append(book_name_tags[i].text.strip())\n",
    "        elif i % 2 != 0:\n",
    "            book_author.append(book_name_tags[i].text.strip())\n",
    "\n",
    "    for i in range(len(book_rating_tag)):\n",
    "        rating_tag = book_rating_tag[i].find_all('span')\n",
    "        book_star.append(rating_tag[0].text.strip())\n",
    "        book_ratings.append(rating_tag[1].text.strip())\n",
    "\n",
    "    for i in range(len(book_type_tag)):\n",
    "        if i % 2 != 0:\n",
    "            type = book_type_tag[1].find_all('span', {'class': 'a-size-small a-color-secondary a-text-normal'})\n",
    "            book_type.append(type[0].text)\n",
    "\n",
    "    for i in range(1, len(book_price_tag)):\n",
    "        if i % 4 == 0:\n",
    "            book_price_tag1 = book_price_tag[i].find_all('span', {'class':'p13n-sc-price'})\n",
    "            if len(book_price_tag1) != 0:\n",
    "                price = book_price_tag1[0].text.strip()\n",
    "                price = price[1:]\n",
    "                book_price.append(price)\n",
    "    #creating dictionary\n",
    "    book_dict = {\n",
    "        'Name' : book_name,\n",
    "        'Author' : book_author,\n",
    "        'No_Of_Ratings' : book_ratings,\n",
    "        'Stars' : book_star,\n",
    "        'Type' : book_type,\n",
    "        'Price' : book_price\n",
    "    }\n",
    "    #Converting dict to Pandas DataFrame\n",
    "    book_df = pd.DataFrame({key:pd.Series(value) for key, value in book_dict.items()})\n",
    "\n",
    "    return book_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c75cd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_books(URL):\n",
    "    os.makedirs('AmazonBestSellers', exist_ok=True)\n",
    "    path = 'AmazonBestSellers/'\n",
    "    \n",
    "    category_df = scrape_category_info(URL)\n",
    "    if os.path.exists(path + 'categories.csv'):\n",
    "            print(\"The file {} already exists. Skipping...\".format(path + 'categories.csv'))\n",
    "    category_df.to_csv('AmazonBestSellers/{}.csv'.format('categories'), index=None)\n",
    "        \n",
    "    for index, row in category_df.iterrows():\n",
    "        if os.path.exists(path + row['name'] + '.csv'):\n",
    "            print(\"The file {} already exists. Skipping...\".format(path + row['name'] + '.csv'))\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['name']))\n",
    "        book_df = book_info(row['url'])\n",
    "        book_df.to_csv(path + '{}.csv'.format(row['name']), index=None)\n",
    "    return print('Scrapping is now complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb1406a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.amazon.in/gp/bestsellers/books/ref=zg_bs_unv_books_1_1318104031_1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestseller_url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_books(bestseller_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
